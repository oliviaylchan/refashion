{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Fashion 2 dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import skimage.io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_loader import load_training_df\n",
    "from masks import get_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_path = 'dataset'\n",
    "output_dir = 'deep_fashion_segmentation_dataset'\n",
    "toy_output_dir = 'deep_fashion_segmentation_dataset_toy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "Read images and annotations in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_training_df(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop non-relevant columns. Note that we decide to use **both shop and user images**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop landmarks as we already have segmentation masks\n",
    "df = df.drop(columns=['scale', 'viewpoint', 'zoom_in', 'landmarks', 'bounding_box', 'occlusion', 'style',  'pair_id', 'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefine categories\n",
    "\n",
    "We saw that some categories are too much fine grained and it would be much useful for us (and easier for a model) to deal with a coarser categorization. Let's map each class to a super-category:\n",
    "\n",
    "- Top: Contains `short sleeve top`, `long sleeve top`, `sling` and `vest`.\n",
    "- Outwear: Contains `short sleeve outwear` and `long sleeve outwear`.\n",
    "- Skirt (`skirt`).\n",
    "- Trousers (`trousers`).\n",
    "- Shorts (`shorts`).\n",
    "- Dress. Contains `vest dress`, `short sleeve dress` and`sling dress`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map categories to supercategory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supercategory_map = {\n",
    "    'short sleeve top': 'top',\n",
    "    'long sleeve dress': 'dress',\n",
    "    'trousers': 'trousers',\n",
    "    'long sleeve top': 'top',\n",
    "    'skirt': 'skirt',\n",
    "    'shorts': 'shorts',\n",
    "    'long sleeve outwear': 'outwear',\n",
    "    'vest dress': 'dress',\n",
    "    'short sleeve dress': 'dress',\n",
    "    'vest': 'top',\n",
    "    'sling dress': 'dress',\n",
    "    'short sleeve outwear': 'outwear',\n",
    "    'sling': 'top'\n",
    "}\n",
    "\n",
    "# Map category to supercategory\n",
    "df.loc[:, 'supercategory_name'] = df['category_name'].map(supercategory_map)\n",
    "\n",
    "# Create id for supercategories\n",
    "# Make sure ids start at 1\n",
    "supercategory_id_map = {\n",
    "    name: i + 1\n",
    "    for i, name in enumerate(set(supercategory_map.values()))\n",
    "}\n",
    "df['supercategory_id'] = df['supercategory_name'].map(supercategory_id_map)\n",
    "\n",
    "# Ensure no extra categories exist\n",
    "assert set(supercategory_map.values()) == set(df['supercategory_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize now the distribution of clothing supercategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['supercategory_name'].value_counts().plot.bar(figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group clothes per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_all_rows(df: pd.DataFrame) -> pd.Series:\n",
    "    result = {col: df[col].values for col in df.columns.values}\n",
    "    # Make sure we keep the id\n",
    "    result.update({'id': df.name})\n",
    "    return result\n",
    "\n",
    "images_series = df.groupby('id').apply(_join_all_rows)\n",
    "images_df = pd.DataFrame.from_records(images_series.values)\n",
    "print(f'Using {len(images_df)} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'images'\n",
    "mask_dir = 'masks'\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'masks'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate path to images and masks in the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask_png_path(files: str) -> str:\n",
    "    basename = os.path.basename(files[0])\n",
    "    filename = os.path.splitext(basename)[0]\n",
    "    return os.path.join(mask_dir, filename + '.png')\n",
    "\n",
    "\n",
    "images_df['source_path'] = images_df['image_path'].apply(\n",
    "    lambda files: os.path.join(image_dir, os.path.basename(files[0]))\n",
    ")\n",
    "\n",
    "images_df.loc[:, 'mask_path'] = images_df['image_path'].apply(build_mask_png_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store picture and mask images for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(supercategory_id_map)\n",
    "\n",
    "def get_mask_image(row: pd.Series) -> np.ndarray:\n",
    "    \n",
    "    n_objects = len(row['segmentation'])\n",
    "    image = skimage.io.imread(row['image_path'][0])\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    \n",
    "    # Build mask matrix\n",
    "    mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "    \n",
    "    # IMPORTANT: order of clothes matter in the resulting mask as\n",
    "    # masks do overlap\n",
    "    for i in range(n_objects):\n",
    "        submask = get_mask(image_height,\n",
    "                           image_width,\n",
    "                           row['segmentation'][i],\n",
    "                           category_id=int(row['supercategory_id'][i]))\n",
    "        mask = np.where(submask != 0, submask, mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def store_example(row: pd.Series, folder: str) -> bool:\n",
    "     \n",
    "    try:\n",
    "        # Copy image\n",
    "        image_dst_path = os.path.join(folder, row['source_path'])\n",
    "        copyfile(row['image_path'][0], image_dst_path)\n",
    "        # Generate and store mask\n",
    "        mask_dst_path = os.path.join(folder, row['mask_path'])\n",
    "        skimage.io.imsave(mask_dst_path, get_mask_image(row))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'Could not store example: {str(e)}')\n",
    "        return False\n",
    "\n",
    "# # Ignore warnings generated when storing mask images\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "images_df['created_ok'] = images_df.apply(\n",
    "    lambda x: store_example(x, output_dir), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate clean CSV\n",
    "\n",
    "Now let's only keep the path to the image and the mask as well as the mapping between supercategories and their ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the rows that raised errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = images_df[images_df['created_ok']==True]\n",
    "print(f'{len(images_df) - len(dataset_df)} images could not be stored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep only columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df[['mask_path', 'source_path', 'supercategory_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.rename(columns={'source_path': 'image_path',\n",
    "                                        'supercategory_id': 'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training-validation-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _row_to_set(row: pd.Series) -> str:\n",
    "    if row.name in train_idx:\n",
    "        return 'train'\n",
    "    elif row.name in val_idx:\n",
    "        return 'validation'\n",
    "    elif row.name in test_idx:\n",
    "        return 'test'\n",
    "    else:\n",
    "        raise ValueError(f'Unknown row index: {row.name}')\n",
    "\n",
    "train_idx, test_idx = train_test_split(dataset_df.index, test_size=0.025, random_state=500)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.0125, random_state=300)\n",
    "\n",
    "dataset_df['set'] = dataset_df.apply(_row_to_set, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize instances per set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['set'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store DataFrame as json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_json(os.path.join(output_dir, 'data.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store supercategory id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_supercategory_id_map = {\n",
    "    category_id: category_name\n",
    "    for category_name, category_id in supercategory_id_map.items()\n",
    "}\n",
    "\n",
    "labels_path = os.path.join(output_dir, 'labels.json')\n",
    "with open(labels_path, 'w') as file:\n",
    "    json.dump(inverse_supercategory_id_map, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy README into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyfile('dataset_README.md', os.path.join(output_dir, 'README.md'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 5\n",
    "fig, axs = plt.subplots(n_rows, 2, figsize=(12, n_rows*4.0))\n",
    "\n",
    "for i in range(n_rows):\n",
    "\n",
    "    sample = dataset_df.sample(1).iloc[0]\n",
    "\n",
    "    # Parse image and mask\n",
    "    image = skimage.io.imread(os.path.join(output_dir, sample['image_path']))\n",
    "    mask = skimage.io.imread(os.path.join(output_dir, sample['mask_path']))\n",
    "    labels = sample['labels']\n",
    "    # Parse labels\n",
    "    str_labels = [inverse_supercategory_id_map[category_id] for category_id in labels]\n",
    "\n",
    "    axs[i][0].imshow(image)\n",
    "    axs[i][1].set_title(f'Categories: {str_labels}')\n",
    "    axs[i][1].imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _category_count(df: pd.DataFrame) -> dict:\n",
    "    label_ids = df['labels'].apply(lambda x: x.tolist()).values\n",
    "    label_ids = list(itertools.chain.from_iterable(label_ids))\n",
    "    label_ids_count = np.unique(label_ids, return_counts=True)\n",
    "    return {\n",
    "        inverse_supercategory_id_map[label_id]: label_count\n",
    "        for label_id, label_count in zip(*label_ids_count)\n",
    "    }\n",
    "\n",
    "print(f'All instances category count: {_category_count(dataset_df)}')\n",
    "print(f'Training set category count: {_category_count(dataset_df[dataset_df[\"set\"] == \"train\"])}')\n",
    "print(f'Validation set category count: {_category_count(dataset_df[dataset_df[\"set\"] == \"validation\"])}')\n",
    "print(f'Test set category count: {_category_count(dataset_df[dataset_df[\"set\"] == \"test\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate toy dataset\n",
    "\n",
    "Build toy dataset to check we can easily overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(toy_output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(toy_output_dir, 'images'))\n",
    "os.makedirs(os.path.join(toy_output_dir, 'masks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only a subset of the images for the toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_images_df = dataset_df[dataset_df['set'] == 'train'].sample(10)\n",
    "toy_images_df.apply(\n",
    "    lambda x: store_example(x, toy_output_dir), axis=1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store toy DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_images_df.to_json(os.path.join(toy_output_dir, 'data.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyfile(labels_path, os.path.join(toy_output_dir, 'labels.json'));"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
