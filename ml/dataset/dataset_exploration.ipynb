{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Fashion 2 visualization\n",
    "\n",
    "The main purpose of this notebook is to understand the contents of the Deep Fashion dataset and have answers to questions like:\n",
    "\n",
    "- How many images per category are there?\n",
    "- How many clothing objects per image are there?\n",
    "- Is the quality of the image good enough?\n",
    "- What does the attributes tell us about the images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import skimage.io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from masks import get_mask\n",
    "from data_loader import load_training_df\n",
    "from visualization import display_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_path = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "\n",
    "Read images and annotations in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_training_df(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map ordinal categories (e.g. scale, occlusion and viewpoint) into categorical columns for better understanding (values according to documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scale_categorical'] = df['scale'].map(\n",
    "    {1: 'small_scale', 2: 'modest_scale', 3: 'large_scale'}\n",
    ")\n",
    "df['zoom_in_categorical'] = df['zoom_in'].map(\n",
    "    {1: 'no_zoom_in', 2: 'medium_zoom_in', 3: 'large_zoom_in'}\n",
    ")\n",
    "df['viewpoint_categorical'] = df['viewpoint'].map(\n",
    "    {1: 'no_wear', 2: 'frontal_viewpoint', 3: 'side_or_back_viewpoint'}\n",
    ")\n",
    "df['occlusion_categorical'] = df['occlusion'].map(\n",
    "    {1: 'slight_occlusion', 2: 'medium_occlusion', 3: 'heavy_occlusion'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe random examples for the different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image by category id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _samples_per_category(df: pd.DataFrame,\n",
    "                          column: str,\n",
    "                          n_samples: int = 7) -> pd.DataFrame:\n",
    "    return df.groupby(column)\\\n",
    "             .apply(lambda x: x.sample(n_samples))\\\n",
    "             .reset_index(level=0, drop=True)\\\n",
    "             .reset_index()\n",
    "    \n",
    "\n",
    "column = 'category_name'\n",
    "display_instances(_samples_per_category(df, column, n_samples=2),\n",
    "                  title_column=column,\n",
    "                  n_cols=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column].value_counts().plot.bar(figsize=(20, 4), rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some categories are highly close to one another (e.g. sling dress and short sleeve dress)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display images by source information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'source'\n",
    "display_instances(_samples_per_category(df, column, n_samples=8),\n",
    "                  title_column=column,\n",
    "                  n_cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column].value_counts().plot.bar(figsize=(10, 4), rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the `shop` images have much higher quality than `user` images. We see that there are ~3 times more shop images than user images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image by viewpoint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'viewpoint_categorical'\n",
    "display_instances(_samples_per_category(df, column, n_samples=4),\n",
    "                  title_column=column,\n",
    "                  n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column].value_counts().plot.bar(figsize=(10, 4), rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the viewpoint information can be ambiguous as side viewpoint (which most of the time are mostly frontal vies) and back viewpoint are tagged in the same category. Moreover, we observe that most of the images fall in frontal category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image by scale information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'scale_categorical'\n",
    "display_instances(_samples_per_category(df, column, n_samples=4),\n",
    "                  title_column=column,\n",
    "                  n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column].value_counts().plot.bar(figsize=(20, 4), rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that scale is not very informative, as `small` and `modest` scale refer to very similar kind of images. However, they seem to properly tag those which are zoomed in pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image by zoom-in information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'zoom_in_categorical'\n",
    "display_instances(_samples_per_category(df, column, n_samples=4),\n",
    "                  title_column=column,\n",
    "                  n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column].value_counts().plot.bar(figsize=(20, 4), rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that this feature gives little information about the content, as we see that similar images appear in different categories such as `no_zoom_in` and `large_zoom_in`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image by occlusion information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'occlusion_categorical'\n",
    "display_instances(_samples_per_category(df, column, n_samples=4),\n",
    "                  title_column=column,\n",
    "                  n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column].value_counts().plot.bar(figsize=(20, 4), rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is not clear what is the criteria used to tag image occlusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ids = list(df.sample(1)['pair_id'].values)\n",
    "sample_pairs_df = df[df['pair_id'].isin(pair_ids)].drop_duplicates(['image_path'])\n",
    "display_instances(sample_pairs_df, title_column='source', n_cols=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shop and user image does not need to be from the same size or color. That can be read in the `style` field (see [documentation](https://github.com/switchablenorms/DeepFashion2))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clothing elements per image: stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes_per_image = df.groupby('image_path')['category_id'].count()\n",
    "mean, std = clothes_per_image.mean(), clothes_per_image.std()\n",
    "print(f'Clothes per image: {mean:.2f} +- {std:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mask image from examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _display_masks(image: np.ndarray, masks: List, ax=None):\n",
    "    # Display image\n",
    "    if ax is not None:\n",
    "        axis = ax\n",
    "    else:\n",
    "        plt.figure(figsize=(8, 15))\n",
    "        axis = plt\n",
    "    axis.imshow(image)\n",
    "    \n",
    "    # Display all masks\n",
    "    for mask in masks:\n",
    "        axis.imshow(mask, alpha=0.25, vmin=-1.0, vmax=1.0)\n",
    "\n",
    "        \n",
    "def instance_to_mask(row: pd.Series) -> np.ndarray:\n",
    "    image = skimage.io.imread(row['image_path'])\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    return get_mask(image_height,\n",
    "                    image_width,\n",
    "                    polygons=row['segmentation'],\n",
    "                    category_id=int(row['category_id']))\n",
    "\n",
    "\n",
    "def display_instance_mask(row: pd.Series, ax) -> None:\n",
    "    masks = [instance_to_mask(row)]\n",
    "    image = skimage.io.imread(row['image_path'])\n",
    "    _display_masks(image, masks, ax=ax)\n",
    "    ax.set_title(row[\"category_name\"])\n",
    "    ax.axis('off')\n",
    "    \n",
    "samples = df.sample(12)\n",
    "display_instances(samples, display_fn=display_instance_mask, n_cols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, in many cases, polygons defining the clothing area are quite sharp and do not properly wrap the clothes margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize examples of images with all masks in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_instance_masks(row: pd.Series, ax) -> None:\n",
    "    items = df[df.index == row.name]\n",
    "    masks = items.apply(instance_to_mask, axis=1).values.tolist()\n",
    "    image = skimage.io.imread(row['image_path'])\n",
    "    _display_masks(image, masks, ax=ax)\n",
    "    # Displau call categories\n",
    "    categories = items[\"category_name\"].values.tolist()\n",
    "    ax.set_title(f'{categories}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "samples = df.sample(12)\n",
    "display_instances(samples, display_fn=display_all_instance_masks, n_cols=6)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
